# -*- coding: utf-8 -*-
"""HealthData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F8YH9-zJHOcs6jJtXX2G3_g4l7gPHGE8
"""

import torch, subprocess, gc
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
print("PyTorch:", torch.__version__, "| CUDA available:", torch.cuda.is_available())
try:
    print(subprocess.check_output(["nvidia-smi"]).decode()[:1000])
except Exception as e:
    print("No GPU? Runtime → Change runtime type → GPU.", e)

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

quant_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    quantization_config=quant_cfg,
    torch_dtype=torch.float16,
)

if tok.pad_token is None:
    tok.pad_token = tok.eos_token

print("Loaded:", MODEL_NAME, "| pad_token_id:", tok.pad_token_id)

import torch

@torch.inference_mode()
def chat_once(system, user, max_new_tokens=260, temperature=0.4):
    messages = [
        {"role": "system", "content": system},
        {"role": "user",   "content": user},
    ]
    prompt = tok.apply_chat_template(
        messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
    ).to(model.device)

    out = model.generate(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=(temperature > 0),
        temperature=temperature,
        top_p=0.9,
        pad_token_id=tok.pad_token_id,
        eos_token_id=tok.eos_token_id,
    )
    gen_ids = out[0][prompt.shape[-1]:]
    return tok.decode(gen_ids, skip_special_tokens=True).strip()

print(chat_once("You are brief.", "Say: ok."))

import random, pathlib
BASE = "/content/dualalign_qwen2_5"
pathlib.Path(BASE).mkdir(parents=True, exist_ok=True)
random.seed(42)

risk_factors = [
    "hypertension","type2_diabetes","obesity","depression","anxiety",
    "hearing_loss","vision_cataract","hyperlipidemia","smoking_history"
]
keywords = {
    "Cognitive": ["forgetfulness","word-finding difficulty","attention lapses","misplacing items"],
    "ConcernsByOthers": ["family reports","spouse noted","caregiver observed"],
    "RequiresAssistance": ["needs help with medications","assistance with finances","supervision for cooking"],
    "Physiological": ["gait instability","insomnia","sleep fragmentation","vision changes"],
    "Neuropsychiatric": ["apathy","irritability","anxiety","depressed mood"]
}

GEN_SYS = (
    "You are a clinician writing concise, realistic SOAP-style notes for longitudinal Alzheimer’s-type care. "
    "Use only the structured inputs; avoid inventing labs/tests. Prefer cautious phrasing ('possible', 'likely'). "
    "Keep timeline coherent with respect to the provided year (negative = years before diagnosis)."
)

TPL = """Patient persona:
- Age/Sex: {age}{sex}
- Risk factors: {risk}
- Year before diagnosis: {year_abs}
- Visit type: {visit}
- Observed keywords (to weave into prose, not bullet copy):
  Cognitive: {Cognitive}
  ConcernsByOthers: {ConcernsByOthers}
  RequiresAssistance: {RequiresAssistance}
  Physiological: {Physiological}
  Neuropsychiatric: {Neuropsychiatric}

Write a brief clinical note (≤220 words) with the headings:
SUBJECTIVE:
OBJECTIVE:
ASSESSMENT:
PLAN:
Use cautious language and precise negation.
"""

def sample_persona():
    return dict(
        age=random.randint(65, 88),
        sex=random.choice(["F","M"]),
        risk=", ".join(random.sample(risk_factors, k=3)),
    )

def sample_keywords():
    return {k: random.sample(v, k=1) for k, v in keywords.items()}

def make_note(pid: int, years):
    p = sample_persona()
    notes = []
    for y in years:
        data = dict(
            year=y, year_abs=abs(y),
            visit=random.choice(["primary_care","neurology","memory_clinic"]),
            **p, **sample_keywords()
        )
        prompt = TPL.format(**data)
        text = chat_once(GEN_SYS, prompt, max_new_tokens=220, temperature=0.2)
        notes.append({"year": y, "note": text})
    return {"patient_id": f"P{pid}", "persona": p, "timeline": notes}

# Smoke test
years = [-6, -2]
patients = [make_note(1, years)]
print(patients[0]["timeline"][0]["note"][:500])

import re, json

VALID_LABELS = ["Cognitive","ConcernsByOthers","RequiresAssistance","Physiological","Neuropsychiatric","None"]

def split_sentences(text: str):

    sents = re.split(r'(?<=[\.\?\!])\s+', text.strip())
    return [s.strip() for s in sents if s.strip()]

def rule_label(s: str) -> str:
    L = s.lower()
    if any(k in L for k in ["forget", "repetition", "word-finding", "lost", "misplac", "attention", "confus"]):
        return "Cognitive"
    if any(k in L for k in ["daughter", "son", "spouse", "wife", "husband", "niece", "nephew", "family", "caregiver"]):
        return "ConcernsByOthers"
    if any(k in L for k in ["needs help", "assistance", "supervision", "remind", "finances", "medication", "appointments"]):
        return "RequiresAssistance"
    if any(k in L for k in ["gait", "walk", "falls", "insomnia", "sleep", "hearing", "vision", "incontinence", "tremor"]):
        return "Physiological"
    if any(k in L for k in ["apathy", "irritability", "agitation", "anxiety", "depressed", "mood", "hallucination", "delusion"]):
        return "Neuropsychiatric"
    return "None"

LABEL_SYS = (
    "You will assign EXACTLY ONE label per sentence from this fixed set: "
    f"{', '.join(VALID_LABELS)}. Output ONLY the labels, one per line, "
    "no numbering, no extra text."
)

def classify_sentences_stable(note: str):
    sents = split_sentences(note)
    if not sents:
        return []

    prompt = (
        "There are {N} sentences. Return exactly {N} lines, each line one label "
        f"from {{{', '.join(VALID_LABELS)}}}.\n\n"
        "Sentences:\n" +
        "\n".join(f"{i+1}. {s}" for i, s in enumerate(sents))
    ).replace("{N}", str(len(sents)))

    resp = chat_once(LABEL_SYS, prompt, max_new_tokens=200, temperature=0.0)

    lines = [ln.strip() for ln in resp.splitlines() if ln.strip()]
    clean = []
    for ln in lines:
        ln = re.sub(r'^\s*\d+[\).\s-]+', '', ln)  # remove "1. " or "1) "
        ln = ln.split()[0] if '\t' not in ln and ':' not in ln else re.split(r'[\t:]', ln, 1)[0]
        if ln in VALID_LABELS:
            clean.append(ln)


    if len(clean) == len(sents):
        return [{"sentence": s, "label": lbl} for s, lbl in zip(sents, clean)]
    else:
        return [{"sentence": s, "label": rule_label(s)} for s in sents]


for t in patients[0]["timeline"]:
    t["labels"] = classify_sentences_stable(t["note"])

print(json.dumps(patients[0]["timeline"][0]["labels"][:3], indent=2))

from tqdm import tqdm
import time, json

years = [-10, -8, -6, -4, -2, -1]
patients = []

for i in tqdm(range(3), desc="Generating patients"):
    p = make_note(i+1, years)
    for t in p["timeline"]:
        t["labels"] = classify_sentences_stable(t["note"])
    patients.append(p)

ts = time.strftime("%Y%m%d-%H%M%S")
out_path = f"{BASE}/dualalign_qwen2_5_7b_{ts}.json"
with open(out_path, "w") as f:
    json.dump(patients, f, indent=2)
print("✅ Saved →", out_path)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

qwen_tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
qwen_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

print("✅ Qwen reloaded:", type(qwen_model))

@torch.inference_mode()
def rag_summary(query_note, retrieved):
    context = "\n".join(
        [f"- ({pid}, year {y}): {txt[:300]}..." for txt, _, (pid, y) in retrieved]
    )
    prompt = f"""
You are a clinical summarizer integrating related case notes.

Query note:
{query_note}

Similar records:
{context}

Task:
1. Provide a short clinical summary synthesizing patterns.
2. Suggest plausible next steps or interventions (non-prescriptive).
3. Keep under 150 words, professional tone.
"""
    messages = [
        {"role": "system", "content": "You are a concise medical summarizer."},
        {"role": "user", "content": prompt},
    ]
    prompt_tensor = qwen_tok.apply_chat_template(
        messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
    ).to(qwen_model.device)

    output = qwen_model.generate(
        prompt_tensor,
        max_new_tokens=220,
        temperature=0.3,
        top_p=0.9,
        pad_token_id=qwen_tok.pad_token_id,
        eos_token_id=qwen_tok.eos_token_id,
    )

    gen = output[0][prompt_tensor.shape[-1]:]
    return qwen_tok.decode(gen, skip_special_tokens=True).strip()


# Test again
sample = patients[0]["timeline"][0]["note"]
retrieved = retrieve_similar_notes(sample)
summary = rag_summary(sample, retrieved)

print("\n=== RAG Clinical Summary ===\n")
print(summary)